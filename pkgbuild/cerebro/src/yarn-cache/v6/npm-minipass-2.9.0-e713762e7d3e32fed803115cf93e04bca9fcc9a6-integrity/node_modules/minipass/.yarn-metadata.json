{
  "manifest": {
    "name": "minipass",
    "version": "2.9.0",
    "description": "minimal implementation of a PassThrough stream",
    "main": "index.js",
    "dependencies": {
      "safe-buffer": "^5.1.2",
      "yallist": "^3.0.0"
    },
    "devDependencies": {
      "end-of-stream": "^1.4.0",
      "tap": "^14.6.5",
      "through2": "^2.0.3"
    },
    "scripts": {
      "test": "tap",
      "preversion": "npm test",
      "postversion": "npm publish",
      "postpublish": "git push origin --follow-tags"
    },
    "repository": {
      "type": "git",
      "url": "git+https://github.com/isaacs/minipass.git"
    },
    "keywords": [
      "passthrough",
      "stream"
    ],
    "author": {
      "name": "Isaac Z. Schlueter",
      "email": "i@izs.me",
      "url": "http://blog.izs.me/"
    },
    "license": "ISC",
    "files": [
      "index.js"
    ],
    "tap": {
      "check-coverage": true
    },
    "_registry": "npm",
    "_loc": "/home/brunomiguel/userrepository/pkgbuild/cerebro/src/yarn-cache/v6/npm-minipass-2.9.0-e713762e7d3e32fed803115cf93e04bca9fcc9a6-integrity/node_modules/minipass/package.json",
    "readmeFilename": "README.md",
    "readme": "# minipass\n\nA _very_ minimal implementation of a [PassThrough\nstream](https://nodejs.org/api/stream.html#stream_class_stream_passthrough)\n\n[It's very\nfast](https://docs.google.com/spreadsheets/d/1oObKSrVwLX_7Ut4Z6g3fZW-AX1j1-k6w-cDsrkaSbHM/edit#gid=0)\nfor objects, strings, and buffers.\n\nSupports pipe()ing (including multi-pipe() and backpressure\ntransmission), buffering data until either a `data` event handler or\n`pipe()` is added (so you don't lose the first chunk), and most other\ncases where PassThrough is a good idea.\n\nThere is a `read()` method, but it's much more efficient to consume\ndata from this stream via `'data'` events or by calling `pipe()` into\nsome other stream.  Calling `read()` requires the buffer to be\nflattened in some cases, which requires copying memory.\n\nThere is also no `unpipe()` method.  Once you start piping, there is\nno stopping it!\n\nIf you set `objectMode: true` in the options, then whatever is written\nwill be emitted.  Otherwise, it'll do a minimal amount of Buffer\ncopying to ensure proper Streams semantics when `read(n)` is called.\n\n`objectMode` can also be set by doing `stream.objectMode = true`, or by\nwriting any non-string/non-buffer data.  `objectMode` cannot be set to\nfalse once it is set.\n\nThis is not a `through` or `through2` stream.  It doesn't transform\nthe data, it just passes it right through.  If you want to transform\nthe data, extend the class, and override the `write()` method.  Once\nyou're done transforming the data however you want, call\n`super.write()` with the transform output.\n\nFor some examples of streams that extend Minipass in various ways, check\nout:\n\n- [minizlib](http://npm.im/minizlib)\n- [fs-minipass](http://npm.im/fs-minipass)\n- [tar](http://npm.im/tar)\n- [minipass-collect](http://npm.im/minipass-collect)\n- [minipass-flush](http://npm.im/minipass-flush)\n- [minipass-pipeline](http://npm.im/minipass-pipeline)\n- [tap](http://npm.im/tap)\n- [tap-parser](http://npm.im/tap)\n- [treport](http://npm.im/tap)\n\n## Differences from Node.js Streams\n\nThere are several things that make Minipass streams different from (and in\nsome ways superior to) Node.js core streams.\n\nPlease read these caveats if you are familiar with noode-core streams and\nintend to use Minipass streams in your programs.\n\n### Timing\n\nMinipass streams are designed to support synchronous use-cases.  Thus, data\nis emitted as soon as it is available, always.  It is buffered until read,\nbut no longer.  Another way to look at it is that Minipass streams are\nexactly as synchronous as the logic that writes into them.\n\nThis can be surprising if your code relies on `PassThrough.write()` always\nproviding data on the next tick rather than the current one, or being able\nto call `resume()` and not have the entire buffer disappear immediately.\n\nHowever, without this synchronicity guarantee, there would be no way for\nMinipass to achieve the speeds it does, or support the synchronous use\ncases that it does.  Simply put, waiting takes time.\n\nThis non-deferring approach makes Minipass streams much easier to reason\nabout, especially in the context of Promises and other flow-control\nmechanisms.\n\n### No High/Low Water Marks\n\nNode.js core streams will optimistically fill up a buffer, returning `true`\non all writes until the limit is hit, even if the data has nowhere to go.\nThen, they will not attempt to draw more data in until the buffer size dips\nbelow a minimum value.\n\nMinipass streams are much simpler.  The `write()` method will return `true`\nif the data has somewhere to go (which is to say, given the timing\nguarantees, that the data is already there by the time `write()` returns).\n\nIf the data has nowhere to go, then `write()` returns false, and the data\nsits in a buffer, to be drained out immediately as soon as anyone consumes\nit.\n\n### Hazards of Buffering (or: Why Minipass Is So Fast)\n\nSince data written to a Minipass stream is immediately written all the way\nthrough the pipeline, and `write()` always returns true/false based on\nwhether the data was fully flushed, backpressure is communicated\nimmediately to the upstream caller.  This minimizes buffering.\n\nConsider this case:\n\n```js\nconst {PassThrough} = require('stream')\nconst p1 = new PassThrough({ highWaterMark: 1024 })\nconst p2 = new PassThrough({ highWaterMark: 1024 })\nconst p3 = new PassThrough({ highWaterMark: 1024 })\nconst p4 = new PassThrough({ highWaterMark: 1024 })\n\np1.pipe(p2).pipe(p3).pipe(p4)\np4.on('data', () => console.log('made it through'))\n\n// this returns false and buffers, then writes to p2 on next tick (1)\n// p2 returns false and buffers, pausing p1, then writes to p3 on next tick (2)\n// p3 returns false and buffers, pausing p2, then writes to p4 on next tick (3)\n// p4 returns false and buffers, pausing p3, then emits 'data' and 'drain'\n// on next tick (4)\n// p3 sees p4's 'drain' event, and calls resume(), emitting 'resume' and\n// 'drain' on next tick (5)\n// p2 sees p3's 'drain', calls resume(), emits 'resume' and 'drain' on next tick (6)\n// p1 sees p2's 'drain', calls resume(), emits 'resume' and 'drain' on next\n// tick (7)\n\np1.write(Buffer.alloc(2048)) // returns false\n```\n\nAlong the way, the data was buffered and deferred at each stage, and\nmultiple event deferrals happened, for an unblocked pipeline where it was\nperfectly safe to write all the way through!\n\nFurthermore, setting a `highWaterMark` of `1024` might lead someone reading\nthe code to think an advisory maximum of 1KiB is being set for the\npipeline.  However, the actual advisory buffering level is the _sum_ of\n`highWaterMark` values, since each one has its own bucket.\n\nConsider the Minipass case:\n\n```js\nconst m1 = new Minipass()\nconst m2 = new Minipass()\nconst m3 = new Minipass()\nconst m4 = new Minipass()\n\nm1.pipe(m2).pipe(m3).pipe(m4)\nm4.on('data', () => console.log('made it through'))\n\n// m1 is flowing, so it writes the data to m2 immediately\n// m2 is flowing, so it writes the data to m3 immediately\n// m3 is flowing, so it writes the data to m4 immediately\n// m4 is flowing, so it fires the 'data' event immediately, returns true\n// m4's write returned true, so m3 is still flowing, returns true\n// m3's write returned true, so m2 is still flowing, returns true\n// m2's write returned true, so m1 is still flowing, returns true\n// No event deferrals or buffering along the way!\n\nm1.write(Buffer.alloc(2048)) // returns true\n```\n\nIt is extremely unlikely that you _don't_ want to buffer any data written,\nor _ever_ buffer data that can be flushed all the way through.  Neither\nnode-core streams nor Minipass ever fail to buffer written data, but\nnode-core streams do a lot of unnecessary buffering and pausing.\n\nAs always, the faster implementation is the one that does less stuff and\nwaits less time to do it.\n\n### Immediately emit `end` for empty streams (when not paused)\n\nIf a stream is not paused, and `end()` is called before writing any data\ninto it, then it will emit `end` immediately.\n\nIf you have logic that occurs on the `end` event which you don't want to\npotentially happen immediately (for example, closing file descriptors,\nmoving on to the next entry in an archive parse stream, etc.) then be sure\nto call `stream.pause()` on creation, and then `stream.resume()` once you\nare ready to respond to the `end` event.\n\n### Emit `end` When Asked\n\nOne hazard of immediately emitting `'end'` is that you may not yet have had\na chance to add a listener.  In order to avoid this hazard, Minipass\nstreams safely re-emit the `'end'` event if a new listener is added after\n`'end'` has been emitted.\n\nIe, if you do `stream.on('end', someFunction)`, and the stream has already\nemitted `end`, then it will call the handler right away.  (You can think of\nthis somewhat like attaching a new `.then(fn)` to a previously-resolved\nPromise.)\n\nTo prevent calling handlers multiple times who would not expect multiple\nends to occur, all listeners are removed from the `'end'` event whenever it\nis emitted.\n\n### Impact of \"immediate flow\" on Tee-streams\n\nA \"tee stream\" is a stream piping to multiple destinations:\n\n```js\nconst tee = new Minipass()\nt.pipe(dest1)\nt.pipe(dest2)\nt.write('foo') // goes to both destinations\n```\n\nSince Minipass streams _immediately_ process any pending data through the\npipeline when a new pipe destination is added, this can have surprising\neffects, especially when a stream comes in from some other function and may\nor may not have data in its buffer.\n\n```js\n// WARNING! WILL LOSE DATA!\nconst src = new Minipass()\nsrc.write('foo')\nsrc.pipe(dest1) // 'foo' chunk flows to dest1 immediately, and is gone\nsrc.pipe(dest2) // gets nothing!\n```\n\nThe solution is to create a dedicated tee-stream junction that pipes to\nboth locations, and then pipe to _that_ instead.\n\n```js\n// Safe example: tee to both places\nconst src = new Minipass()\nsrc.write('foo')\nconst tee = new Minipass()\ntee.pipe(dest1)\ntee.pipe(dest2)\nstream.pipe(tee) // tee gets 'foo', pipes to both locations\n```\n\nThe same caveat applies to `on('data')` event listeners.  The first one\nadded will _immediately_ receive all of the data, leaving nothing for the\nsecond:\n\n```js\n// WARNING! WILL LOSE DATA!\nconst src = new Minipass()\nsrc.write('foo')\nsrc.on('data', handler1) // receives 'foo' right away\nsrc.on('data', handler2) // nothing to see here!\n```\n\nUsing a dedicated tee-stream can be used in this case as well:\n\n```js\n// Safe example: tee to both data handlers\nconst src = new Minipass()\nsrc.write('foo')\nconst tee = new Minipass()\ntee.on('data', handler1)\ntee.on('data', handler2)\nsrc.pipe(tee)\n```\n\n## USAGE\n\nIt's a stream!  Use it like a stream and it'll most likely do what you want.\n\n```js\nconst Minipass = require('minipass')\nconst mp = new Minipass(options) // optional: { encoding, objectMode }\nmp.write('foo')\nmp.pipe(someOtherStream)\nmp.end('bar')\n```\n\n### OPTIONS\n\n* `encoding` How would you like the data coming _out_ of the stream to be\n  encoded?  Accepts any values that can be passed to `Buffer.toString()`.\n* `objectMode` Emit data exactly as it comes in.  This will be flipped on\n  by default if you write() something other than a string or Buffer at any\n  point.  Setting `objectMode: true` will prevent setting any encoding\n  value.\n\n### API\n\nImplements the user-facing portions of Node.js's `Readable` and `Writable`\nstreams.\n\n### Methods\n\n* `write(chunk, [encoding], [callback])` - Put data in.  (Note that, in the\n  base Minipass class, the same data will come out.)  Returns `false` if\n  the stream will buffer the next write, or true if it's still in\n  \"flowing\" mode.\n* `end([chunk, [encoding]], [callback])` - Signal that you have no more\n  data to write.  This will queue an `end` event to be fired when all the\n  data has been consumed.\n* `setEncoding(encoding)` - Set the encoding for data coming of the\n  stream.  This can only be done once.\n* `pause()` - No more data for a while, please.  This also prevents `end`\n  from being emitted for empty streams until the stream is resumed.\n* `resume()` - Resume the stream.  If there's data in the buffer, it is\n  all discarded.  Any buffered events are immediately emitted.\n* `pipe(dest)` - Send all output to the stream provided.  There is no way\n  to unpipe.  When data is emitted, it is immediately written to any and\n  all pipe destinations.\n* `on(ev, fn)`, `emit(ev, fn)` - Minipass streams are EventEmitters.\n  Some events are given special treatment, however.  (See below under\n  \"events\".)\n* `promise()` - Returns a Promise that resolves when the stream emits\n  `end`, or rejects if the stream emits `error`.\n* `collect()` - Return a Promise that resolves on `end` with an array\n  containing each chunk of data that was emitted, or rejects if the\n  stream emits `error`.  Note that this consumes the stream data.\n* `concat()` - Same as `collect()`, but concatenates the data into a\n  single Buffer object.  Will reject the returned promise if the stream is\n  in objectMode, or if it goes into objectMode by the end of the data.\n* `read(n)` - Consume `n` bytes of data out of the buffer.  If `n` is not\n  provided, then consume all of it.  If `n` bytes are not available, then\n  it returns null.  **Note** consuming streams in this way is less\n  efficient, and can lead to unnecessary Buffer copying.\n* `destroy([er])` - Destroy the stream.  If an error is provided, then an\n  `'error'` event is emitted.  If the stream has a `close()` method, and\n  has not emitted a `'close'` event yet, then `stream.close()` will be\n  called.  Any Promises returned by `.promise()`, `.collect()` or\n  `.concat()` will be rejected.  After being destroyed, writing to the\n  stream will emit an error.  No more data will be emitted if the stream is\n  destroyed, even if it was previously buffered.\n\n### Properties\n\n* `bufferLength` Read-only.  Total number of bytes buffered, or in the case\n  of objectMode, the total number of objects.\n* `encoding` The encoding that has been set.  (Setting this is equivalent\n  to calling `setEncoding(enc)` and has the same prohibition against\n  setting multiple times.)\n* `flowing` Read-only.  Boolean indicating whether a chunk written to the\n  stream will be immediately emitted.\n* `emittedEnd` Read-only.  Boolean indicating whether the end-ish events\n  (ie, `end`, `prefinish`, `finish`) have been emitted.  Note that\n  listening on any end-ish event will immediateyl re-emit it if it has\n  already been emitted.\n* `writable` Whether the stream is writable.  Default `true`.  Set to\n  `false` when `end()`\n* `readable` Whether the stream is readable.  Default `true`.\n* `buffer` A [yallist](http://npm.im/yallist) linked list of chunks written\n  to the stream that have not yet been emitted.  (It's probably a bad idea\n  to mess with this.)\n* `pipes` A [yallist](http://npm.im/yallist) linked list of streams that\n  this stream is piping into.  (It's probably a bad idea to mess with\n  this.)\n* `destroyed` A getter that indicates whether the stream was destroyed.\n* `paused` True if the stream has been explicitly paused, otherwise false.\n* `objectMode` Indicates whether the stream is in `objectMode`.  Once set\n  to `true`, it cannot be set to `false`.\n\n### Events\n\n* `data` Emitted when there's data to read.  Argument is the data to read.\n  This is never emitted while not flowing.  If a listener is attached, that\n  will resume the stream.\n* `end` Emitted when there's no more data to read.  This will be emitted\n  immediately for empty streams when `end()` is called.  If a listener is\n  attached, and `end` was already emitted, then it will be emitted again.\n  All listeners are removed when `end` is emitted.\n* `prefinish` An end-ish event that follows the same logic as `end` and is\n  emitted in the same conditions where `end` is emitted.  Emitted after\n  `'end'`.\n* `finish` An end-ish event that follows the same logic as `end` and is\n  emitted in the same conditions where `end` is emitted.  Emitted after\n  `'prefinish'`.\n* `close` An indication that an underlying resource has been released.\n  Minipass does not emit this event, but will defer it until after `end`\n  has been emitted, since it throws off some stream libraries otherwise.\n* `drain` Emitted when the internal buffer empties, and it is again\n  suitable to `write()` into the stream.\n* `readable` Emitted when data is buffered and ready to be read by a\n  consumer.\n* `resume` Emitted when stream changes state from buffering to flowing\n  mode.  (Ie, when `resume` is called, `pipe` is called, or a `data` event\n  listener is added.)\n\n### Static Methods\n\n* `Minipass.isStream(stream)` Returns `true` if the argument is a stream,\n  and false otherwise.  To be considered a stream, the object must be\n  either an instance of Minipass, or an EventEmitter that has either a\n  `pipe()` method, or both `write()` and `end()` methods.  (Pretty much any\n  stream in node-land will return `true` for this.)\n\n## EXAMPLES\n\nHere are some examples of things you can do with Minipass streams.\n\n### simple \"are you done yet\" promise\n\n```js\nmp.promise().then(() => {\n  // stream is finished\n}, er => {\n  // stream emitted an error\n})\n```\n\n### collecting\n\n```js\nmp.collect().then(all => {\n  // all is an array of all the data emitted\n  // encoding is supported in this case, so\n  // so the result will be a collection of strings if\n  // an encoding is specified, or buffers/objects if not.\n  //\n  // In an async function, you may do\n  // const data = await stream.collect()\n})\n```\n\n### collecting into a single blob\n\nThis is a bit slower because it concatenates the data into one chunk for\nyou, but if you're going to do it yourself anyway, it's convenient this\nway:\n\n```js\nmp.concat().then(onebigchunk => {\n  // onebigchunk is a string if the stream\n  // had an encoding set, or a buffer otherwise.\n})\n```\n\n### iteration\n\nYou can iterate over streams synchronously or asynchronously in\nplatforms that support it.\n\nSynchronous iteration will end when the currently available data is\nconsumed, even if the `end` event has not been reached.  In string and\nbuffer mode, the data is concatenated, so unless multiple writes are\noccurring in the same tick as the `read()`, sync iteration loops will\ngenerally only have a single iteration.\n\nTo consume chunks in this way exactly as they have been written, with\nno flattening, create the stream with the `{ objectMode: true }`\noption.\n\n```js\nconst mp = new Minipass({ objectMode: true })\nmp.write('a')\nmp.write('b')\nfor (let letter of mp) {\n  console.log(letter) // a, b\n}\nmp.write('c')\nmp.write('d')\nfor (let letter of mp) {\n  console.log(letter) // c, d\n}\nmp.write('e')\nmp.end()\nfor (let letter of mp) {\n  console.log(letter) // e\n}\nfor (let letter of mp) {\n  console.log(letter) // nothing\n}\n```\n\nAsynchronous iteration will continue until the end event is reached,\nconsuming all of the data.\n\n```js\nconst mp = new Minipass({ encoding: 'utf8' })\n\n// some source of some data\nlet i = 5\nconst inter = setInterval(() => {\n  if (i --> 0)\n    mp.write(Buffer.from('foo\\n', 'utf8'))\n  else {\n    mp.end()\n    clearInterval(inter)\n  }\n}, 100)\n\n// consume the data with asynchronous iteration\nasync function consume () {\n  for await (let chunk of mp) {\n    console.log(chunk)\n  }\n  return 'ok'\n}\n\nconsume().then(res => console.log(res))\n// logs `foo\\n` 5 times, and then `ok`\n```\n\n### subclass that `console.log()`s everything written into it\n\n```js\nclass Logger extends Minipass {\n  write (chunk, encoding, callback) {\n    console.log('WRITE', chunk, encoding)\n    return super.write(chunk, encoding, callback)\n  }\n  end (chunk, encoding, callback) {\n    console.log('END', chunk, encoding)\n    return super.end(chunk, encoding, callback)\n  }\n}\n\nsomeSource.pipe(new Logger()).pipe(someDest)\n```\n\n### same thing, but using an inline anonymous class\n\n```js\n// js classes are fun\nsomeSource\n  .pipe(new (class extends Minipass {\n    emit (ev, ...data) {\n      // let's also log events, because debugging some weird thing\n      console.log('EMIT', ev)\n      return super.emit(ev, ...data)\n    }\n    write (chunk, encoding, callback) {\n      console.log('WRITE', chunk, encoding)\n      return super.write(chunk, encoding, callback)\n    }\n    end (chunk, encoding, callback) {\n      console.log('END', chunk, encoding)\n      return super.end(chunk, encoding, callback)\n    }\n  }))\n  .pipe(someDest)\n```\n\n### subclass that defers 'end' for some reason\n\n```js\nclass SlowEnd extends Minipass {\n  emit (ev, ...args) {\n    if (ev === 'end') {\n      console.log('going to end, hold on a sec')\n      setTimeout(() => {\n        console.log('ok, ready to end now')\n        super.emit('end', ...args)\n      }, 100)\n    } else {\n      return super.emit(ev, ...args)\n    }\n  }\n}\n```\n\n### transform that creates newline-delimited JSON\n\n```js\nclass NDJSONEncode extends Minipass {\n  write (obj, cb) {\n    try {\n      // JSON.stringify can throw, emit an error on that\n      return super.write(JSON.stringify(obj) + '\\n', 'utf8', cb)\n    } catch (er) {\n      this.emit('error', er)\n    }\n  }\n  end (obj, cb) {\n    if (typeof obj === 'function') {\n      cb = obj\n      obj = undefined\n    }\n    if (obj !== undefined) {\n      this.write(obj)\n    }\n    return super.end(cb)\n  }\n}\n```\n\n### transform that parses newline-delimited JSON\n\n```js\nclass NDJSONDecode extends Minipass {\n  constructor (options) {\n    // always be in object mode, as far as Minipass is concerned\n    super({ objectMode: true })\n    this._jsonBuffer = ''\n  }\n  write (chunk, encoding, cb) {\n    if (typeof chunk === 'string' &&\n        typeof encoding === 'string' &&\n        encoding !== 'utf8') {\n      chunk = Buffer.from(chunk, encoding).toString()\n    } else if (Buffer.isBuffer(chunk))\n      chunk = chunk.toString()\n    }\n    if (typeof encoding === 'function') {\n      cb = encoding\n    }\n    const jsonData = (this._jsonBuffer + chunk).split('\\n')\n    this._jsonBuffer = jsonData.pop()\n    for (let i = 0; i < jsonData.length; i++) {\n      let parsed\n      try {\n        super.write(parsed)\n      } catch (er) {\n        this.emit('error', er)\n        continue\n      }\n    }\n    if (cb)\n      cb()\n  }\n}\n```\n",
    "licenseText": "The ISC License\n\nCopyright (c) npm, Inc. and Contributors\n\nPermission to use, copy, modify, and/or distribute this software for any\npurpose with or without fee is hereby granted, provided that the above\ncopyright notice and this permission notice appear in all copies.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\nWITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\nANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\nWHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\nACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR\nIN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n"
  },
  "artifacts": [],
  "remote": {
    "resolved": "https://registry.yarnpkg.com/minipass/-/minipass-2.9.0.tgz#e713762e7d3e32fed803115cf93e04bca9fcc9a6",
    "type": "tarball",
    "reference": "https://registry.yarnpkg.com/minipass/-/minipass-2.9.0.tgz",
    "hash": "e713762e7d3e32fed803115cf93e04bca9fcc9a6",
    "integrity": "sha512-wxfUjg9WebH+CUDX/CdbRlh5SmfZiy/hpkxaRI16Y9W56Pa75sWgd/rvFilSgrauD9NyFymP/+JFV3KwzIsJeg==",
    "registry": "npm",
    "packageName": "minipass",
    "cacheIntegrity": "sha512-wxfUjg9WebH+CUDX/CdbRlh5SmfZiy/hpkxaRI16Y9W56Pa75sWgd/rvFilSgrauD9NyFymP/+JFV3KwzIsJeg== sha1-5xN2Ln0+Mv7YAxFc+T4EvKn8yaY="
  },
  "registry": "npm",
  "hash": "e713762e7d3e32fed803115cf93e04bca9fcc9a6"
}